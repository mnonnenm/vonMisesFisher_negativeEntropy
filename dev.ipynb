{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "from exps import load_classic3_sklearn, load_news20_sklearn\n",
    "from vMFne.bregman_clustering import posterior_marginal_vMF_mixture_Ψ\n",
    "from vMFne.moVMF import posterior_marginal_vMF_mixture_Φ\n",
    "\n",
    "Ψ0 = [None, 0.]\n",
    "\n",
    "# Calculate MI between two clusterings\n",
    "def mi(class_true, class_est):\n",
    "    cont = metrics.cluster.contingency_matrix(class_true, class_est, sparse=True).astype(np.float64, copy=False)\n",
    "    mi = metrics.cluster.mutual_info_score(class_true, class_est, contingency=cont)\n",
    "    return mi\n",
    "\n",
    "def get_MIs(fn_rootroot, fn_roots, n_repets, K_range, class_true, NMI=False):\n",
    "    MIs = np.zeros((len(fn_roots), n_repets, len(K_range)))\n",
    "    comp_mi = metrics.normalized_mutual_info_score if NMI else mi\n",
    "    for f,fn_root in enumerate(fn_roots):\n",
    "        for k,K in enumerate(K_range):\n",
    "            if f == 0:\n",
    "                fn = fn_rootroot + fn_root + str(K) + '.npy'\n",
    "                out = np.load(fn, allow_pickle=True)\n",
    "                for i in range(n_repets):\n",
    "                    class_est = out[i]\n",
    "                    MIs[f,i,k] =  comp_mi(class_true.flatten(), class_est.flatten())\n",
    "            elif f in [1,2,5,6]:\n",
    "                fn = fn_rootroot + fn_root + str(K) + '.npz'\n",
    "                out = np.load(fn, allow_pickle=True)['out'].tolist()\n",
    "                for i in range(n_repets):\n",
    "                    ph_x_μ, _ = posterior_marginal_vMF_mixture_Ψ(X,out['w'][i],out['μs'][i], Ψ0=Ψ0)\n",
    "                    class_est = np.argmax(ph_x_μ,axis=1)\n",
    "                    MIs[f,i,k] = comp_mi(class_true.flatten(), class_est.flatten()) \n",
    "            elif f in [3,4,7,8]:\n",
    "                fn = fn_rootroot + fn_root + str(K) + '.npz'\n",
    "                out = np.load(fn, allow_pickle=True)['out'].tolist()\n",
    "                for i in range(n_repets):\n",
    "                    ph_x_η, _ = posterior_marginal_vMF_mixture_Φ(X,out['w'][i],out['ηs'][i])\n",
    "                    class_est = np.argmax(ph_x_η,axis=1)\n",
    "                    MIs[f,i,k] = comp_mi(class_true.flatten(), class_est.flatten()) \n",
    "    return MIs\n",
    "\n",
    "def get_fn_roots(n_repets, version):\n",
    "    return [f'spkmeans_{n_repets}repets_seed_0_v{version}__K_',\n",
    "            f'softBregClust_{n_repets}repets_seed_0_no_tying__v{version}__K_',\n",
    "            f'softBregClust_{n_repets}repets_seed_0_with_tying__v{version}__K_',\n",
    "            f'softmovMF_{n_repets}repets_seed_0_no_tying__v{version}__K_',\n",
    "            f'softmovMF_{n_repets}repets_seed_0_with_tying__v{version}__K_',\n",
    "            f'hardBregClust_{n_repets}repets_seed_0_no_tying__v{version}__K_',\n",
    "            f'hardBregClust_{n_repets}repets_seed_0_with_tying__v{version}__K_',\n",
    "            f'hardmovMF_{n_repets}repets_seed_0_no_tying__v{version}__K_',\n",
    "            f'hardmovMF_{n_repets}repets_seed_0_with_tying__v{version}__K_']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_rootroot = 'results/classic3_'\n",
    "K_range = np.arange(2,11,1)\n",
    "n_repets = 10\n",
    "version = 0\n",
    "seed = 0\n",
    "\n",
    "X, labels, dictionary = load_classic3_sklearn()\n",
    "class_true = sum([ (1.*i) * (labels==np.unique(labels)[i]) for i in range(len(np.unique(labels)))])\n",
    "\n",
    "fn_roots = get_fn_roots(n_repets, version)\n",
    "MIs_classic3 = get_MIs(fn_rootroot, fn_roots, n_repets, K_range, class_true, NMI=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "clrs = ['b', 'magenta', 'magenta', 'black', 'black', 'orange', 'orange', 'green', 'green' ]\n",
    "lnstl = ['-', '--', '-', '--', '-', '--', '-', '--', '-']\n",
    "mrkrs = ['o','o','o','.','.','o','o','.','.']\n",
    "for f in range(len(fn_roots)):\n",
    "    plt.plot(K_range, MIs_classic3.mean(axis=1)[f], label=fn_roots[f], \n",
    "             color=clrs[f], linestyle=lnstl[f], marker=mrkrs[f])\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('NMI')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', MIs_classic3)\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', \n",
    "        {'MIs' : MIs_classic3,\n",
    "         'algorithms' : fn_roots,\n",
    "         'K_range' : K_range,\n",
    "         'seed' : seed,\n",
    "         'n_repets' : n_repets,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549844b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_rootroot = 'results/classic300_'\n",
    "K_range = np.arange(2,11,1)\n",
    "n_repets = 10\n",
    "version = 0\n",
    "seed = 0\n",
    "\n",
    "X, labels, dictionary = load_classic3_sklearn(classic300=True, min_df=2, max_df=0.15)\n",
    "class_true = sum([ (1.*i) * (labels==np.unique(labels)[i]) for i in range(len(np.unique(labels)))])\n",
    "\n",
    "fn_roots = get_fn_roots(n_repets, version)\n",
    "MIs_classic300 = get_MIs(fn_rootroot, fn_roots, n_repets, K_range, class_true, NMI=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "clrs = ['b', 'magenta', 'magenta', 'black', 'black', 'orange', 'orange', 'green', 'green' ]\n",
    "lnstl = ['-', '--', '-', '--', '-', '--', '-', '--', '-']\n",
    "mrkrs = ['o','o','o','.','.','o','o','.','.']\n",
    "for f in range(len(fn_roots)):\n",
    "    plt.plot(K_range, MIs_classic300.mean(axis=1)[f], label=fn_roots[f], \n",
    "             color=clrs[f], linestyle=lnstl[f], marker=mrkrs[f])\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('NMI')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', MIs_classic300)\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', \n",
    "        {'MIs' : MIs_classic300,\n",
    "         'algorithms' : fn_roots,\n",
    "         'K_range' : K_range,\n",
    "         'seed' : seed,\n",
    "         'n_repets' : n_repets,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f285849",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_rootroot = 'results/news20_'\n",
    "K_range = np.arange(4,41,4)\n",
    "n_repets = 10\n",
    "version = 0\n",
    "seed = 0\n",
    "\n",
    "X, labels, dictionary = load_news20_sklearn()\n",
    "class_true = sum([ (1.*i) * (labels==np.unique(labels)[i]) for i in range(len(np.unique(labels)))])\n",
    "\n",
    "fn_roots = get_fn_roots(n_repets, version)\n",
    "MIs_news20 = get_MIs(fn_rootroot, fn_roots, n_repets, K_range, class_true, NMI=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "clrs = ['b', 'magenta', 'magenta', 'black', 'black', 'orange', 'orange', 'green', 'green' ]\n",
    "lnstl = ['-', '--', '-', '--', '-', '--', '-', '--', '-']\n",
    "mrkrs = ['o','o','o','.','.','o','o','.','.']\n",
    "for f in range(len(fn_roots)):\n",
    "    plt.plot(K_range, MIs_news20.mean(axis=1)[f], label=fn_roots[f], \n",
    "             color=clrs[f], linestyle=lnstl[f], marker=mrkrs[f])\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('NMI')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', MIs_news20)\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', \n",
    "        {'MIs' : MIs_news20,\n",
    "         'algorithms' : fn_roots,\n",
    "         'K_range' : K_range,\n",
    "         'seed' : seed,\n",
    "         'n_repets' : n_repets,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac90a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_rootroot = 'results/news20small_'\n",
    "K_range = np.arange(4,41,4)\n",
    "n_repets = 10\n",
    "version = 0\n",
    "seed = 0\n",
    "X, labels, dictionary = load_news20_sklearn(news20_small=True, min_df=2, max_df=0.15)\n",
    "class_true = sum([ (1.*i) * (labels==np.unique(labels)[i]) for i in range(len(np.unique(labels)))])\n",
    "\n",
    "fn_roots = get_fn_roots(n_repets, version)\n",
    "MIs_news20small = get_MIs(fn_rootroot, fn_roots, n_repets, K_range, class_true, NMI=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "clrs = ['b', 'magenta', 'magenta', 'black', 'black', 'orange', 'orange', 'green', 'green' ]\n",
    "lnstl = ['-', '--', '-', '--', '-', '--', '-', '--', '-']\n",
    "mrkrs = ['o','o','o','.','.','o','o','.','.']\n",
    "for f in range(len(fn_roots)):\n",
    "    plt.plot(K_range, MIs_news20small.mean(axis=1)[f], label=fn_roots[f], \n",
    "             color=clrs[f], linestyle=lnstl[f], marker=mrkrs[f])\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('NMI')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', MIs_news20small)\n",
    "np.save(f'{fn_rootroot}MIs_{n_repets}repets_seed_0_v{version}__K{min(K_range)}_{max(K_range)}', \n",
    "        {'MIs' : MIs_news20small,\n",
    "         'algorithms' : fn_roots,\n",
    "         'K_range' : K_range,\n",
    "         'seed' : seed,\n",
    "         'n_repets' : n_repets,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c3c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from exps import run_all_classic3\n",
    "\n",
    "version = '0'\n",
    "classic300 = True\n",
    "\n",
    "run_all_classic3(fn_root='results/classic300_', n_repets=10, K_range=[2,3,4,5,6,7,8,9,10,11], \n",
    "                 seed=0, max_iter=100, κ_max=10000., Ψ0=[None, 0.], version=version, \n",
    "                 classic300=classic300, verbose=True, min_df=2, max_df=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106be7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exps import run_all_news20\n",
    "\n",
    "version = '0'\n",
    "news20_small = True\n",
    "\n",
    "run_all_news20(fn_root='results/news20small_', n_repets=10, K_range=[4,8,12,16,20,24,28,32,36,40], \n",
    "                 seed=0, max_iter=100, κ_max=10000., Ψ0=[None, 0.], version=version, \n",
    "                 news20_small=news20_small, verbose=True, min_df=2, max_df=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33994e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exps import run_all_classic3\n",
    "\n",
    "version = '0'\n",
    "classic300 = False\n",
    "\n",
    "run_all_classic3(fn_root='results/classic3_', n_repets=10, K_range=[2,3,4,5,6,7,8,9,10,11], \n",
    "                 seed=0, max_iter=100, κ_max=10000., Ψ0=[None, 0.], version=version, \n",
    "                 classic300=classic300, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc452579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exps import run_all_news20\n",
    "\n",
    "version = '0'\n",
    "news20_small = False\n",
    "\n",
    "run_all_news20(fn_root='results/news20_', n_repets=10, K_range=[4,8,12,16,20,24,28,32,36,40], \n",
    "                 seed=0, max_iter=100, κ_max=10000., Ψ0=[None, 0.], version=version, \n",
    "                 news20_small=news20_small, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8af055",
   "metadata": {},
   "source": [
    "# numerical evaluation of the Negentropy computation and approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vMFne.logpartition import gradΦ, invgradΦ, vMF_entropy_Φ, logχ, log_besseli, banerjee_44\n",
    "from vMFne.negentropy import Ψ, Ψ_base, gradΨ, dΨ_base\n",
    "from scipy.special import loggamma\n",
    "import mpmath\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Ds = [2, 10, 100, 1000]\n",
    "\n",
    "t0 = 0.0\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "for i,D in enumerate(Ds):\n",
    "\n",
    "    #Ψ0 = [0., invgradΦ(np.array([t0]),D,max_iter=10, atol=1e-12)[0][0]]\n",
    "    Ψ0 = [0., 1e-6]\n",
    "\n",
    "    K = 100\n",
    "    V = np.ones((K,D))/np.sqrt(D)\n",
    "\n",
    "    μs_norm = np.linspace(0, 1, K+2)[1:-1]\n",
    "    _, gradΨμ = Ψ(μs_norm.reshape(-1,1)*V, D, Ψ0=Ψ0, t0=t0, return_grad=True)\n",
    "\n",
    "    κs = np.linalg.norm(gradΨμ,axis=-1)\n",
    "\n",
    "    # re-compute ||μ|| = Φ'(||η||) to make sure H[η] is not at a disadvantage because η are bad- if anything, μ are bad ! \n",
    "    μs_norm = np.linalg.norm(gradΦ(κs.reshape(-1,1)*V),axis=-1)\n",
    "    Ψμ, gradΨμ = Ψ(μs_norm.reshape(-1,1)*V, D, Ψ0=Ψ0, t0=t0, return_grad=True, solve_delta=False)\n",
    "    atol = 1e-12\n",
    "    κs_est, diffs = invgradΦ(μs_norm,D,max_iter=10,atol=atol)\n",
    "    κs_est_0, diffs = invgradΦ(μs_norm,D,max_iter=0,atol=atol)\n",
    "\n",
    "    Ψμ_delta, gradΨμ_delta = Ψ(μs_norm.reshape(-1,1)*V, D, Ψ0=Ψ0, t0=t0, return_grad=True)\n",
    "\n",
    "    # NegEntropy\n",
    "    plt.subplot(2,2,1)\n",
    "    H = vMF_entropy_Φ(κs_est.reshape(-1,1) * V)\n",
    "    Ψμ0 = (D/2. - 1.) * np.log(2) + loggamma(D/2)\n",
    "    negH = - H - logχ(D) - Ψμ0\n",
    "    plt.plot(μs_norm, negH, '-', label='D='+str(D))\n",
    "    plt.plot(μs_norm, Ψ_base(μs_norm,D=D), '--')\n",
    "    plt.plot(μs_norm, Ψμ_delta, ':')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(μs_norm, (negH - Ψμ), ':', label='D='+str(D))\n",
    "    plt.plot(μs_norm, (negH - Ψ_base(μs_norm,D=D)), '--')\n",
    "    plt.plot(μs_norm, (negH - Ψμ_delta), ':')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gradient of NegEntropy\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(μs_norm, np.linalg.norm(gradΨμ,axis=-1), label='D='+str(D))\n",
    "    plt.plot(μs_norm, dΨ_base(μs_norm,D=D), '--')\n",
    "    plt.plot(μs_norm, np.linalg.norm(gradΨμ_delta,axis=-1), ':')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    #plt.plot(μs_norm, np.linalg.norm(gradΨμ,axis=-1) - banerjee_44(μs_norm,D=D), ':',\n",
    "    #             label='D='+str(D))\n",
    "    plt.plot(μs_norm, np.linalg.norm(gradΨμ,axis=-1) - dΨ_base(μs_norm,D=D), '--',\n",
    "                 label='D='+str(D))\n",
    "    plt.plot(μs_norm, np.linalg.norm(gradΨμ,axis=-1) - np.linalg.norm(gradΨμ_delta,axis=-1), ':')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(μs_norm[:-1], np.diff(np.linalg.norm(gradΨμ,axis=-1) - banerjee_44(μs_norm,D=D))/ np.diff(μs_norm)[0] + μs_norm[:-1], ':')\n",
    "plt.plot(μs_norm[:-1], 0.08 * np.sin(2*np.pi*μs_norm[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vMFne.logpartition import gradΦ, invgradΦ, vMF_entropy_Φ, logχ, log_besseli, banerjee_44\n",
    "from vMFne.negentropy import Ψ, Ψ_base, gradΨ, dΨ_base\n",
    "from scipy.special import loggamma\n",
    "import mpmath\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Ds = [10, 100, 1000, 25000]\n",
    "\n",
    "t0 = 0.0\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "for i,D in enumerate(Ds):\n",
    "\n",
    "    Ψ0 = [0., 1e-4]\n",
    "\n",
    "    K = 100\n",
    "    V = np.ones((K,D))/np.sqrt(D)\n",
    "\n",
    "    μs_norm = np.linspace(0, 1, K+2)[1:-1]\n",
    "    Ψμ, gradΨμ = Ψ(μs_norm.reshape(-1,1)*V, D, Ψ0=Ψ0, t0=t0, return_grad=True, solve_delta=False)\n",
    "    Ψμ_delta, gradΨμ_delta = Ψ(μs_norm.reshape(-1,1)*V, D, Ψ0=Ψ0, t0=t0, return_grad=True, solve_delta=True)\n",
    "\n",
    "    # NegEntropy\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(μs_norm, Ψμ, label='D='+str(D))\n",
    "    plt.plot(μs_norm, Ψ_base(μs_norm,D=D), '--')\n",
    "    plt.plot(μs_norm, Ψμ_delta, ':')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(μs_norm, (Ψμ - Ψ_base(μs_norm,D=D)) / Ψμ, '--', label='D='+str(D))\n",
    "    plt.plot(μs_norm, (Ψμ - Ψμ_delta) / Ψμ, ':')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gradient of NegEntropy\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(μs_norm, np.linalg.norm(gradΨμ,axis=-1), label='D='+str(D))\n",
    "    plt.plot(μs_norm, dΨ_base(μs_norm,D=D), '--')\n",
    "    plt.plot(μs_norm, np.linalg.norm(gradΨμ_delta,axis=-1), ':')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(μs_norm, (np.linalg.norm(gradΨμ,axis=-1) - dΨ_base(μs_norm,D=D)) / np.linalg.norm(gradΨμ,axis=-1), '--',\n",
    "                 label='D='+str(D))\n",
    "    plt.plot(μs_norm, (np.linalg.norm(gradΨμ,axis=-1) - np.linalg.norm(gradΨμ_delta,axis=-1))/np.linalg.norm(gradΨμ,axis=-1) , ':')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vMFne.logpartition import gradΦ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "N = 3891\n",
    "D = 4255\n",
    "\n",
    "Ψ0 = [None, 0.]\n",
    "\n",
    "#weights = np.array([0.10, 0.15, 0.2, 0.25, 0.3])\n",
    "#kappas = D/50. * np.array([1., 10., 15., 25., 0.1])\n",
    "\n",
    "weights = np.array([0.26548445, 0.37522488, 0.35929067])\n",
    "kappas = D/50. * np.array([8., 10., 11.])\n",
    "\n",
    "Ns = np.int32(np.round(N*weights))\n",
    "K = len(weights)\n",
    "class_true = np.concatenate([k * np.ones(Ns[k]) for k in range(K)])\n",
    "\n",
    "mus = np.random.normal(size=(K,D))\n",
    "mus = mus / np.linalg.norm(mus,axis=-1).reshape(-1,1)\n",
    "mu_norms = np.linalg.norm(gradΦ(kappas.reshape(-1,1) * mus), axis=-1)\n",
    "\n",
    "vmf = sample_vMF_Ulrich if D==3 else ()\n",
    "x_vmf = []\n",
    "for Nk,muk,kappak in zip(Ns, mus, kappas):\n",
    "    x_vmf.append(scipy.stats.vonmises_fisher(mu=muk, kappa=kappak).rvs(Nk))\n",
    "x_vmf = np.concatenate(x_vmf, axis=0)\n",
    "\n",
    "if D == 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from sklearn.metrics import pairwise\n",
    "    from matplotlib.pyplot import cm\n",
    "\n",
    "    fig = plt.figure()\n",
    "    u = np.linspace( 0, 2 * np.pi, 60)\n",
    "    v = np.linspace( 0, np.pi, 30 )\n",
    "\n",
    "    # create the sphere surface\n",
    "    XX = np.outer( np.cos( u ), np.sin( v ) )\n",
    "    YY = np.outer( np.sin( u ), np.sin( v ) )\n",
    "    ZZ = np.outer( np.ones( np.size( u ) ), np.cos( v ) )\n",
    "    locs = np.stack([XX.flatten(),YY.flatten(),ZZ.flatten()], axis=-1)\n",
    "\n",
    "    d0 = 0.1\n",
    "    WW_vmf = (pairwise.pairwise_distances(locs, x_vmf)<d0).sum(axis=-1)\n",
    "    myheatmap_vmf = WW_vmf.reshape(len(u), len(v)) / WW_vmf.max()\n",
    "    ax = fig.add_subplot( 1, 2, 2, projection='3d')\n",
    "    ax.plot_surface( XX, YY,  ZZ, cstride=1, rstride=1, facecolors=cm.jet( myheatmap_vmf ) )\n",
    "    plt.title('von Mises-Fisher')\n",
    "    plt.show()\n",
    "\n",
    "from vMFne.moVMF import posterior_marginal_vMF_mixture_Φ\n",
    "from vMFne.bregman_clustering import posterior_marginal_vMF_mixture_Ψ\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def mi(class_true, class_est):\n",
    "    cont = metrics.cluster.contingency_matrix(class_true, class_est, sparse=True).astype(np.float64, copy=False)\n",
    "    mi = metrics.cluster.mutual_info_score(class_true, class_est, contingency=cont)\n",
    "    return mi\n",
    "\n",
    "ph_x_μ_true_Ψ, log_px_true_Ψ = posterior_marginal_vMF_mixture_Ψ(x_vmf,weights,mu_norms.reshape(-1,1)*mus, Ψ0=Ψ0)\n",
    "LL_true_Ψ = log_px_true_Ψ.sum()\n",
    "\n",
    "if D <= 1000:\n",
    "    ph_x_μ_true_Φ, log_px_true_Φ = posterior_marginal_vMF_mixture_Φ(x_vmf,weights,kappas.reshape(-1,1)*mus)\n",
    "    LL_true_Φ = log_px_true_Φ.sum()\n",
    "    class_est_μ_true = np.argmax(ph_x_μ_true_Φ,axis=1)\n",
    "    LL_true = LL_true_Φ\n",
    "else:\n",
    "    class_est_μ_true = np.argmax(ph_x_μ_true_Ψ,axis=1)\n",
    "    LL_true = LL_true_Ψ\n",
    "M_μ_true = confusion_matrix(class_true, class_est_μ_true)\n",
    "\n",
    "plt.imshow(M_μ_true)\n",
    "plt.colorbar()\n",
    "plt.title('learned model - LL= ' + str(LL_true))\n",
    "plt.ylabel('MI=' + str(mi(class_true.flatten(), class_est_μ_true.flatten())))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "if D <= 1000:\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.loglog(log_px_true_Ψ, log_px_true_Φ, '.')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.loglog(ph_x_μ_true_Ψ.flatten(), ph_x_μ_true_Φ.flatten(), '.')\n",
    "    plt.subplot(1,3,1)\n",
    "plt.plot(kappas, mu_norms.flatten(), 'o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vMFne.bregman_clustering import spherical_kmeans, softBregmanClustering_vMF\n",
    "from vMFne.bregman_clustering import posterior_marginal_vMF_mixture_Ψ\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "\n",
    "all_μs, all_w, all_LL = [], [], []\n",
    "all_μs_kmean, all_w_kmean, all_c_kmean = [], [], []\n",
    "\n",
    "n_repets = 10\n",
    "for ii in range(n_repets):\n",
    "    _, w, c = spherical_kmeans(X=x_vmf, K=K, max_iter=100, verbose=False)\n",
    "    μs = np.stack([x_vmf[c==k].mean(axis=0) for k in range(K)],axis=0)\n",
    "    all_c_kmean.append(1 * c)\n",
    "    all_w_kmean.append(1. * w)\n",
    "    all_μs_kmean.append(1. * μs)\n",
    "    μs, w, LL = softBregmanClustering_vMF(X=x_vmf, K=K, max_iter=100, w_init=w, μs_init=μs, Ψ0=Ψ0, verbose=False)\n",
    "\n",
    "    all_μs.append(μs)\n",
    "    all_w.append(w)\n",
    "    all_LL.append(LL)\n",
    "    print(' - ' + str(ii+1) + '/' + str(n_repets))\n",
    "\n",
    "plt.plot(np.stack(all_LL).T)\n",
    "plt.plot([0, len(LL)], [LL_true, LL_true], 'k--')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "\n",
    "MIs = np.zeros(n_repets)\n",
    "for i in range(n_repets):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    #class_est = all_c_kmean[i]\n",
    "    μs = all_μs_kmean[i]\n",
    "    w = all_w_kmean[i]\n",
    "    ph_x, px = posterior_marginal_vMF_mixture_Ψ(x_vmf,w,μs, Ψ0=Ψ0)\n",
    "    class_est = np.argmax(ph_x,axis=1)\n",
    "    M = confusion_matrix(class_true, class_est)\n",
    "    _, idx_class_align = scipy.optimize.linear_sum_assignment(-M.T)\n",
    "    class_est_aligned = idx_class_align[class_est]\n",
    "    M = confusion_matrix(class_true, class_est_aligned)    \n",
    "    plt.imshow(M)\n",
    "    plt.colorbar()\n",
    "    plt.title('learned model - sph. K-means')\n",
    "    MIs[i] = mi(class_true.flatten(), class_est.flatten()) \n",
    "    plt.ylabel('MI=' + str(MIs[i]))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    w = all_w[i]\n",
    "    μs = all_μs[i]\n",
    "    LL = all_LL[i]\n",
    "    ph_x, px = posterior_marginal_vMF_mixture_Ψ(x_vmf,w,μs, Ψ0=Ψ0)\n",
    "    class_est = np.argmax(ph_x,axis=1)\n",
    "    M = confusion_matrix(class_true, class_est)\n",
    "    _, idx_class_align = scipy.optimize.linear_sum_assignment(-M.T)\n",
    "    class_est_aligned = idx_class_align[class_est]\n",
    "    M = confusion_matrix(class_true, class_est_aligned)    \n",
    "    plt.imshow(M)\n",
    "    plt.colorbar()\n",
    "    plt.title('learned model - LL= ' + str(LL[-1]))\n",
    "    MIs[i] = mi(class_true.flatten(), class_est.flatten()) \n",
    "    plt.ylabel('MI=' + str(MIs[i]))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plt.plot(np.stack(all_LL,axis=0)[:,-1], MIs, 'o')\n",
    "plt.plot([np.stack(all_LL,axis=0)[:,-1].min(), np.stack(all_LL,axis=0)[:,-1].max()], \n",
    "          np.ones(2) * mi(class_true.flatten(), class_est_μ_true.flatten()), 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454c3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f96e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666116f4",
   "metadata": {},
   "source": [
    "# towards mean-parameterized (Hyper-)spherical VAEs\n",
    "Quick idea to make something out of mean parameterization for hyperspherical VAEs:\n",
    "- hyperspherical VAEs are defined by von Mises-Fisher p(z), q(z|x) and general (typically Gaussian) p(x|z).\n",
    "- as such they require the reparametrization trick to get training gradients for q(z|x) from the ELBO\n",
    "- reparametrizaition for von Mises-Fisher latents is known, but is i) cumbersome and ii) formulated in natural parameterization (one samples a univariate $\\omega \\sim p(\\omega \\ | \\ \\kappa = ||\\eta||, D)$.\n",
    "- we here try a quick idea for $D=2$ and $D=3$ whereafter one only approximately samples $q(z|x)$ by sampling $\\tilde{z} \\sim \\mathcal{N}(\\tilde{z}| \\mu(x), \\sigma_\\mu^2)$, where $\\mu(x)$ is the mean parameter of the vMF $q(z|x)$. Then $z = \\tilde{z}/||\\tilde{z}||$, which is differentiable almost surely. The question is for the best-approximating variance function $\\sigma^2_\\mu$, i.e. a function in $\\mu(x)$ (or more sensibly in $||\\mu(x)||$).\n",
    "- for $D=2,3$, it seems that $\\sigma^2_\\mu = \\frac{1-||\\mu||^{(8-2D))}}{\\sqrt(2\\pi)}$ works quite well. \n",
    "- generalization to $D > 3$ currently unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import vonmises  \n",
    "from matplotlib.pyplot import cm\n",
    "from vMFne.utils_angular import cart2spherical, spherical_rotMat\n",
    "from vMFne.sample import sample_vMF_Ulrich\n",
    "\n",
    "D = 3\n",
    "N = 1000000\n",
    "\n",
    "def sigma2(norm_mu):\n",
    "    c = 2\n",
    "    renorm =  1./np.sqrt(2*np.pi) * (1 - norm_mu**((4-D)*c))\n",
    "    return renorm\n",
    "\n",
    "mu_base = np.array([0., 0.0, 1.0])[-D:].reshape(1,D)\n",
    "mu_base = mu_base / np.sqrt( (mu_base**2).sum() )\n",
    "mu_norms = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "plt.figure(figsize=(8,16))\n",
    "for jj in range(len(mu_norms)):\n",
    "    mu = mu_norms[jj] * mu_base\n",
    "    norm_mu = np.sqrt( (mu**2).sum() )\n",
    "\n",
    "    # numerically approximate \\grad\\Psi(||mu||) = ||eta|| = kappa\n",
    "    etas = np.linspace(0,100, 100000)\n",
    "    target = norm_mu\n",
    "    kappa = np.linalg.norm(gradΨ(μ=mu,D=D))\n",
    "\n",
    "    # sample from Gaussian proposal\n",
    "    renorm = sigma2(norm_mu)\n",
    "    x = mu + np.random.normal(size=[N,D]) * np.sqrt(renorm)\n",
    "    x_norm = x / np.sqrt((x**2).sum(axis=-1)).reshape(-1,1)\n",
    "\n",
    "    phi_x = cart2spherical(x.T)\n",
    "\n",
    "    plt.subplot(np.int32(np.ceil(len(mu_norms)/2.)), 2, jj+1)\n",
    "    if D == 2:\n",
    "        xx = np.linspace(-np.pi, np.pi, 100)\n",
    "        phi = cart2spherical(x.T)\n",
    "        h_x,bins_x = np.histogram(phi, bins=xx, density=True)\n",
    "        phi_mu = np.arctan2(mu[...,1], mu[...,0])\n",
    "        phi_vmf = np.mod(vonmises.rvs(kappa, size=N) + phi_mu + np.pi, 2*np.pi) - np.pi\n",
    "        plt.hist(phi_vmf, bins=xx, density=True)\n",
    "        plt.plot(bins_x[:-1]+np.diff(bins_x[:2])[0]/2, h_x)\n",
    "    elif D == 3:\n",
    "        xx = np.linspace(0, np.pi, 50)        \n",
    "        x_vmf = sample_vMF_Ulrich(N=N, m=mu.flatten()/norm_mu, kappa=kappa)\n",
    "        phi_vmf = np.mod(cart2spherical(x_vmf.T) + np.pi, 2*np.pi) - np.pi\n",
    "        h_vmf,_ = np.histogram(phi_vmf[0], xx, density=True)\n",
    "        h_x,_   = np.histogram(phi_x[0], xx, density=True)        \n",
    "        plt.plot(xx[:-1] + (xx[1]-xx[0])/2., h_x, label='angles of Gaussian draws')\n",
    "        plt.plot(xx[:-1] + (xx[1]-xx[0])/2., h_vmf, label='von Mises-Fisher distribution')\n",
    "    plt.title(r'$||\\mu||=' + \"{:10.2f}\".format(norm_mu) + ', \\kappa=' + \"{:10.2f}\".format(kappa) + '$')\n",
    "    if jj == 0:\n",
    "        plt.ylabel('radial profiles of angles')\n",
    "        plt.legend()\n",
    "\n",
    "    \"\"\"\n",
    "    For 3D plotting (plotting on S^2 in 3D plots), code adapted from\n",
    "    https://stackoverflow.com/questions/22128909/plotting-the-temperature-distribution-on-a-sphere-with-python\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from sklearn.metrics import pairwise\n",
    "\n",
    "    if D == 3:\n",
    "        fig = plt.figure()\n",
    "\n",
    "        u = np.linspace( 0, 2 * np.pi, 120)\n",
    "        v = np.linspace( 0, np.pi, 60 )\n",
    "\n",
    "        # create the sphere surface\n",
    "        XX = np.outer( np.cos( u ), np.sin( v ) )\n",
    "        YY = np.outer( np.sin( u ), np.sin( v ) )\n",
    "        ZZ = np.outer( np.ones( np.size( u ) ), np.cos( v ) )\n",
    "        locs = np.stack([XX.flatten(),YY.flatten(),ZZ.flatten()], axis=-1)\n",
    "\n",
    "        d0 = 0.1\n",
    "        WW_vmf = (pairwise.pairwise_distances(locs, x_vmf.T)<d0).sum(axis=-1)\n",
    "        myheatmap_vmf = WW_vmf.reshape(len(u), len(v)) / WW_vmf.max()\n",
    "        WW_x = (pairwise.pairwise_distances(locs, x_norm.T)<d0).sum(axis=-1)\n",
    "        myheatmap_x = WW_vmf.reshape(len(u), len(v)) / WW_x.max()\n",
    "\n",
    "        # ~ ax.scatter( *zip( *pointList ), color='#dd00dd' )\n",
    "        ax = fig.add_subplot( 1, 2, 1, projection='3d')\n",
    "        ax.plot_surface( XX, YY,  ZZ, cstride=1, rstride=1, facecolors=cm.jet( myheatmap_x ) )\n",
    "        plt.title('angles of Gaussian')\n",
    "\n",
    "        ax = fig.add_subplot( 1, 2, 2, projection='3d')\n",
    "        ax.plot_surface( XX, YY,  ZZ, cstride=1, rstride=1, facecolors=cm.jet( myheatmap_vmf ) )\n",
    "        plt.title('von Mises-Fisher')\n",
    "        plt.show() \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2f141",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d342d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
