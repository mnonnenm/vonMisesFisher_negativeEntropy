{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyreadr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# load dataset\n",
    "\n",
    "classic3 = pyreadr.read_r('data/classic3.RData')['classic3']\n",
    "\n",
    "X_raw = classic3.to_numpy()\n",
    "N, D_raw = X_raw.shape\n",
    "labels = X_raw[:,-1]\n",
    "X_raw = X_raw[:,1:-1].astype(dtype=np.float32)\n",
    "\n",
    "# tfn scheme - normalized term frequency-inverse document frequency\n",
    "\n",
    "Nj = (X_raw > 0).sum(axis=0) # number of documents containing word j = 1, ..., D\n",
    "idx = np.where(Nj > 0)[0] # remove dead features\n",
    "D = len(idx)\n",
    "\n",
    "sub_sample_features = True\n",
    "if sub_sample_features:\n",
    "    D_max = 200\n",
    "    D = np.min((D,D_max))\n",
    "    idx__ = np.argsort(Nj)\n",
    "    idx = idx__[-D:]\n",
    "\n",
    "X_raw, Nj = X_raw[:,idx], Nj[idx]\n",
    "\n",
    "gj = np.log(N/Nj)\n",
    "si = 1. / np.sqrt(((X_raw * gj.reshape(1,D))**2).sum(axis=1))\n",
    "\n",
    "X = X_raw * np.outer(si, gj)\n",
    "\n",
    "plt.hist(np.mean(X_raw==0., axis=1), density=True)\n",
    "plt.xlabel('sparisty of vectors')\n",
    "plt.ylabel('rel. frequency in dataset')\n",
    "plt.show()\n",
    "\n",
    "print('\\n')\n",
    "print('selecting D=' + str(D) + ' features out of ' + str(D_raw) + ' features in full dataset.')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 'true' parameters using known labels\n",
    "from vMFne.negentropy import gradΨ\n",
    "from vMFne.bregman_clustering import posterior_marginal_vMF_mixture_Ψ\n",
    "from vMFne.moVMF import posterior_marginal_vMF_mixture_Φ\n",
    "\n",
    "ls = np.unique(labels)\n",
    "μs_true = np.stack([ np.mean(X[np.where(labels==ls[k])[0]],axis=0) for k in range(len(ls)) ], axis=0)\n",
    "w_true = np.array([ np.sum(labels==ls[k]) for k in range(len(ls))]) / len(labels)\n",
    "w_true = w_true / w_true.sum()\n",
    "print(w_true)\n",
    "μs_norm = np.linalg.norm(μs_true,axis=1) \n",
    "print(μs_true.dot(μs_true.T))\n",
    "print(μs_true.dot(μs_true.T) / np.outer(μs_norm,μs_norm))\n",
    "\n",
    "_, px_true_Ψ = posterior_marginal_vMF_mixture_Ψ(X,w_true,μs_true)\n",
    "LL_true_Ψ = np.log(px_true_Ψ).sum()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_true = sum([ (1.*i) * (labels==ls[i]) for i in range(len(ls))])\n",
    "ph_x_μ_true, _ = posterior_marginal_vMF_mixture_Ψ(X,w_true,μs_true)\n",
    "class_est_μ_true = np.argmax(ph_x_μ_true,axis=1)\n",
    "M_μ_true = confusion_matrix(class_true, class_est_μ_true)\n",
    "\n",
    "ηs_true = gradΨ(μs_true,D=D)\n",
    "_, px_true_Φ = posterior_marginal_vMF_mixture_Φ(X,w_true,ηs_true)\n",
    "LL_true_Φ = np.log(px_true_Φ).sum() # may differ from LL_true_Ψ by a constant offset\n",
    "\n",
    "plt.plot(μs_true.T)\n",
    "plt.title('mean parameters per class')\n",
    "plt.xlabel('# of feature')\n",
    "plt.ylabel('μ[# of feature]')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cf335",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vMFne.moVMF import moVMF, posterior_marginal_vMF_mixture_Φ\n",
    "\n",
    "ηs, w, LL = moVMF(X, K=3, max_iter=200, verbose=False, ηs_init = ηs_true, w_init = w_true)\n",
    "ph_x, px = posterior_marginal_vMF_mixture_Φ(X,w,ηs)\n",
    "\n",
    "plt.plot(LL)\n",
    "plt.plot([0, len(LL)], [LL_true_Φ,LL_true_Φ], 'k--')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "class_est = np.argmax(ph_x,axis=1)\n",
    "M = confusion_matrix(class_true, class_est)\n",
    "plt.imshow(M)\n",
    "plt.colorbar()\n",
    "plt.title('learned model')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(M_μ_true)\n",
    "plt.colorbar()\n",
    "plt.title('supervised model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00148fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vMFne.bregman_clustering import softBregmanClustering_vMF\n",
    "from vMFne.bregman_clustering import posterior_marginal_vMF_mixture_Ψ\n",
    "\n",
    "μs, w, LL = softBregmanClustering_vMF(X, K=3, max_iter=200, verbose=False, μs_init = μs_true, w_init = w_true)\n",
    "\n",
    "plt.plot(LL)\n",
    "plt.plot([0, len(LL)], [LL_true_Ψ,LL_true_Ψ], 'k--')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "\n",
    "ph_x, px = posterior_marginal_vMF_mixture_Ψ(X,w,μs)\n",
    "\n",
    "print('w', w)\n",
    "print(μs.dot(μs.T))\n",
    "norm_μs = np.linalg.norm(μs, axis=1)\n",
    "print(μs.dot(μs.T) / np.outer(norm_μs,norm_μs))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "class_est = np.argmax(ph_x,axis=1)\n",
    "M = confusion_matrix(class_true, class_est)\n",
    "plt.imshow(M)\n",
    "plt.colorbar()\n",
    "plt.title('learned model')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(M_μ_true)\n",
    "plt.colorbar()\n",
    "plt.title('supervised model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 20\n",
    "κs = np.linspace(10, 500, 50)\n",
    "ηs = np.ones((len(κs), D)) / np.sqrt(D)\n",
    "ηs = κs.reshape(-1,1) * ηs\n",
    "\n",
    "def grad_Φ(ηs):\n",
    "    ηs = np.atleast_2d(ηs)\n",
    "    K,D = ηs.shape\n",
    "    κs = np.linalg.norm(ηs,axis=-1)    \n",
    "    IeD2 = scipy.special.ive(D/2.,κs)\n",
    "    IeD2_1 = scipy.special.ive(D/2.-1.,κs)\n",
    "    μs = ηs * ((IeD2/IeD2_1) / κs).reshape(-1,1)\n",
    "    return μs\n",
    "\n",
    "from vMFne.negentropy import Ψ\n",
    "\n",
    "μs = grad_Φ(ηs)\n",
    "H_Φ = vMF_entropy_Φ(ηs)\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "H_vmf = np.zeros_like(κs)\n",
    "for i,κ in enumerate(κs):\n",
    "    H_vmf[i] = scipy.stats.vonmises_fisher.entropy(kappa=κ, mu=np.ones(D)/np.sqrt(D))\n",
    "\n",
    "plt.plot(H_Φ.flatten(), -Ψ(μs, D=D))\n",
    "plt.plot(H_Φ.flatten(), H_vmf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ea941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f96e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666116f4",
   "metadata": {},
   "source": [
    "# towards mean-parameterized (Hyper-)spherical VAEs\n",
    "Quick idea to make something out of mean parameterization for hyperspherical VAEs:\n",
    "- hyperspherical VAEs are defined by von Mises-Fisher p(z), q(z|x) and general (typically Gaussian) p(x|z).\n",
    "- as such they require the reparametrization trick to get training gradients for q(z|x) from the ELBO\n",
    "- reparametrizaition for von Mises-Fisher latents is known, but is i) cumbersome and ii) formulated in natural parameterization (one samples a univariate $\\omega \\sim p(\\omega \\ | \\ \\kappa = ||\\eta||, D)$.\n",
    "- we here try a quick idea for $D=2$ and $D=3$ whereafter one only approximately samples $q(z|x)$ by sampling $\\tilde{z} \\sim \\mathcal{N}(\\tilde{z}| \\mu(x), \\sigma_\\mu^2)$, where $\\mu(x)$ is the mean parameter of the vMF $q(z|x)$. Then $z = \\tilde{z}/||\\tilde{z}||$, which is differentiable almost surely. The question is for the best-approximating variance function $\\sigma^2_\\mu$, i.e. a function in $\\mu(x)$ (or more sensibly in $||\\mu(x)||$).\n",
    "- for $D=2,3$, it seems that $\\sigma^2_\\mu = \\frac{1-||\\mu||^{(8-2D))}}{\\sqrt(2\\pi)}$ works quite well. \n",
    "- generalization to $D > 3$ currently unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac37c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import vonmises  \n",
    "from matplotlib.pyplot import cm\n",
    "from vMFne.utils_angular import cart2spherical, spherical_rotMat\n",
    "from vMFne.sample import sample_vMF_Ulrich\n",
    "\n",
    "D = 3\n",
    "N = 1000000\n",
    "\n",
    "def sigma2(norm_mu):\n",
    "    c = 2\n",
    "    renorm =  1./np.sqrt(2*np.pi) * (1 - norm_mu**((4-D)*c))\n",
    "    return renorm\n",
    "\n",
    "mu_base = np.array([0., 0.0, 1.0])[-D:].reshape(1,D)\n",
    "mu_base = mu_base / np.sqrt( (mu_base**2).sum() )\n",
    "mu_norms = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "plt.figure(figsize=(8,16))\n",
    "for jj in range(len(mu_norms)):\n",
    "    mu = mu_norms[jj] * mu_base\n",
    "    norm_mu = np.sqrt( (mu**2).sum() )\n",
    "\n",
    "    # numerically approximate \\grad\\Psi(||mu||) = ||eta|| = kappa\n",
    "    etas = np.linspace(0,100, 100000)\n",
    "    target = norm_mu\n",
    "    kappa = np.linalg.norm(gradΨ(μ=mu,D=D))\n",
    "\n",
    "    # sample from Gaussian proposal\n",
    "    renorm = sigma2(norm_mu)\n",
    "    x = mu + np.random.normal(size=[N,D]) * np.sqrt(renorm)\n",
    "    x_norm = x / np.sqrt((x**2).sum(axis=-1)).reshape(-1,1)\n",
    "\n",
    "    phi_x = cart2spherical(x.T)\n",
    "\n",
    "    plt.subplot(np.int32(np.ceil(len(mu_norms)/2.)), 2, jj+1)\n",
    "    if D == 2:\n",
    "        xx = np.linspace(-np.pi, np.pi, 100)\n",
    "        phi = cart2spherical(x.T)\n",
    "        h_x,bins_x = np.histogram(phi, bins=xx, density=True)\n",
    "        phi_mu = np.arctan2(mu[...,1], mu[...,0])\n",
    "        phi_vmf = np.mod(vonmises.rvs(kappa, size=N) + phi_mu + np.pi, 2*np.pi) - np.pi\n",
    "        plt.hist(phi_vmf, bins=xx, density=True)\n",
    "        plt.plot(bins_x[:-1]+np.diff(bins_x[:2])[0]/2, h_x)\n",
    "    elif D == 3:\n",
    "        xx = np.linspace(0, np.pi, 50)        \n",
    "        x_vmf = sample_vMF_Ulrich(N=N, m=mu.flatten()/norm_mu, kappa=kappa)\n",
    "        phi_vmf = np.mod(cart2spherical(x_vmf.T) + np.pi, 2*np.pi) - np.pi\n",
    "        h_vmf,_ = np.histogram(phi_vmf[0], xx, density=True)\n",
    "        h_x,_   = np.histogram(phi_x[0], xx, density=True)        \n",
    "        plt.plot(xx[:-1] + (xx[1]-xx[0])/2., h_x, label='angles of Gaussian draws')\n",
    "        plt.plot(xx[:-1] + (xx[1]-xx[0])/2., h_vmf, label='von Mises-Fisher distribution')\n",
    "    plt.title(r'$||\\mu||=' + \"{:10.2f}\".format(norm_mu) + ', \\kappa=' + \"{:10.2f}\".format(kappa) + '$')\n",
    "    if jj == 0:\n",
    "        plt.ylabel('radial profiles of angles')\n",
    "        plt.legend()\n",
    "\n",
    "    \"\"\"\n",
    "    For 3D plotting (plotting on S^2 in 3D plots), code adapted from\n",
    "    https://stackoverflow.com/questions/22128909/plotting-the-temperature-distribution-on-a-sphere-with-python\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from sklearn.metrics import pairwise\n",
    "\n",
    "    if D == 3:\n",
    "        fig = plt.figure()\n",
    "\n",
    "        u = np.linspace( 0, 2 * np.pi, 120)\n",
    "        v = np.linspace( 0, np.pi, 60 )\n",
    "\n",
    "        # create the sphere surface\n",
    "        XX = np.outer( np.cos( u ), np.sin( v ) )\n",
    "        YY = np.outer( np.sin( u ), np.sin( v ) )\n",
    "        ZZ = np.outer( np.ones( np.size( u ) ), np.cos( v ) )\n",
    "        locs = np.stack([XX.flatten(),YY.flatten(),ZZ.flatten()], axis=-1)\n",
    "\n",
    "        d0 = 0.1\n",
    "        WW_vmf = (pairwise.pairwise_distances(locs, x_vmf.T)<d0).sum(axis=-1)\n",
    "        myheatmap_vmf = WW_vmf.reshape(len(u), len(v)) / WW_vmf.max()\n",
    "        WW_x = (pairwise.pairwise_distances(locs, x_norm.T)<d0).sum(axis=-1)\n",
    "        myheatmap_x = WW_vmf.reshape(len(u), len(v)) / WW_x.max()\n",
    "\n",
    "        # ~ ax.scatter( *zip( *pointList ), color='#dd00dd' )\n",
    "        ax = fig.add_subplot( 1, 2, 1, projection='3d')\n",
    "        ax.plot_surface( XX, YY,  ZZ, cstride=1, rstride=1, facecolors=cm.jet( myheatmap_x ) )\n",
    "        plt.title('angles of Gaussian')\n",
    "\n",
    "        ax = fig.add_subplot( 1, 2, 2, projection='3d')\n",
    "        ax.plot_surface( XX, YY,  ZZ, cstride=1, rstride=1, facecolors=cm.jet( myheatmap_vmf ) )\n",
    "        plt.title('von Mises-Fisher')\n",
    "        plt.show() \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87ff8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
